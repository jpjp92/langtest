# TODO: 평가 데이터 파이프라인 고도화 및 증강 전략

현재 구축된 LLM-as-a-Judge 평가 파이프라인의 정확도와 신뢰성을 높이기 위해서는 **질이 높고 다양한 평가 데이터(Golden Dataset)**를 지속적으로 확보하는 것이 필수적입니다. 아래는 향후 진행할 데이터 추가 및 증강(Augmentation) 방안입니다.

---

## 1. 평가용 데이터 추가 방안 (Data Collection Strategy)

### 📌 A. 프로덕션 로그 기반 수집 (Human-in-the-Loop)
- **개요**: 실제 서비스에서 사용자와 AI 에이전트가 나눈 대화 중 품질이 높은 데이터를 평가 데이터로 편입시킵니다.
- **구현 방안**:
  - 사용자 피드백(👍/👎 버튼) 연동: 사용자가 '좋아요'를 누른 답변(또는 관리자가 채택한 답변)의 `질문(Question)`, `답변(Expected Answer)`, `검색된 도구/문맥(Context)`을 Supabase `evaluation_dataset` 테이블에 자동 보관하는 로직 추가.
  - 정기 모니터링: 주기적으로 LangSmith 등 트레이싱 도구를 확인하여 잘못된 답변이 나온 케이스(Edge case)를 수동으로 발췌하고, 정답(Expected Answer)을 교정하여 오답 노트 형식으로 DB에 추가.

### 📌 B. 도메인 전문가 중심의 케이스 큐레이션
- **개요**: 시스템이 커버하기 어려운 예외/복합 상황(Edge Cases)을 수동으로 구성합니다.
- **주요 타겟 상황**:
  - 구독 요금제 예약 변경 + 위약금/잔여금 일할 계산이 겹치는 복잡한 산수 요청
  - 명확히 시스템 권한 밖의 질문 (예: "경쟁사 요금제가 더 좋아보이는데?")에 대한 적절한 방어(거절) 답변

---

## 2. 데이터 증강 방안 (Data Augmentation Strategy)

현재 17개의 뼈대 질문을 수백 개 규모로 확장하기 위해 LLM을 동원한 데이터 증강을 진행합니다.

### 📌 A. 발화 스타일 및 어조 변환 (Paraphrasing & Tone Shift)
- **개요**: 동일한 의도(Intent)의 질문을 다양한 화법으로 재생산합니다.
- **LLM 프롬프트 전략 예시**:
  - "다음 질문을 10대 은어/줄임말을 섞어서 다시 써줘."
  - "이 질문을 화가 난 고객의 클레임 톤으로 다급하게 바꿔봐."
  - "오타 및 문법적 오류를 2군데 의도적으로 포함시켜줘."

### 📌 B. 메타데이터 변수 치환 (Variable Swap / Template Method)
- **개요**: 기본 뼈대 문장에 들어가는 주요 조건 변수(요금제명, 날짜, 예산액)를 무작위로 교체합니다.
- **적용 예시**:
  - 원본: "예산 **20만원**으로 맞춰서 **에센셜** 요금제 추천해줘."
  - 변형: "예산 **5만원**으로 맞춰서 **라이트** 요금제 추천해줘."

### 📌 C. 멀티턴 대화 증강 (Multi-turn Context Simulation)
- **개요**: 단발성 질문(Single-turn)을 넘어, 여러 번 주고받는 대화 안에서의 평가 데이터를 만듭니다.
- **증강 시나리오**:
  1. (Turn 1 질문) "요금제 제일 싼 거 뭐야?"
  2. (Turn 1 응답) "라이트 모델 9900원입니다."
  3. **(Turn 2 질문 - 증강 목적)** "그거 말고 그 다음 거는?" -> *에이전트가 이전 컨텍스트(요금제 비교)를 기억하는지 묻는 고난이도 데이터셋 생성*

---

## 🚀 다음 실행 계획 (Next Steps)
- [ ] `generate_qa.py` 스크립트를 업그레이드하여, 기존 DB 데이터를 읽어와서 `Paraphrasing` 증강을 거친 뒤 다시 삽입하는 기능 추가.
- [ ] Frontend 채팅창에 👍/👎 피드백 버튼 추가 후 Backend API로 데이터 전송 파이프라인 구성.
