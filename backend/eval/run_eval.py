import os
import json
import asyncio
from dotenv import load_dotenv
from supabase import create_client, Client
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.panel import Panel
from rich.table import Table
from deepeval.test_case import LLMTestCase

import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from eval_metrics import (
    groundedness_metric,
    evidenceability_metric,
    clarity_metric,
    atomicity_metric,
    robustness_metric
)

# ë°±ì—”ë“œ API ëŒ€ì‹  LangGraph ì—ì´ì „íŠ¸ ëª¨ë“ˆ ì§ì ‘ ì—°ë™
from backend.main import app_graph, HumanMessage, AIMessage

load_dotenv()
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
console = Console()

def fetch_eval_data_from_db():
    response = supabase.table("evaluation_dataset").select("*").execute()
    return response.data

async def generate_agent_response(question: str, user_id: str) -> str:
    """Agentì— ë¹„ë™ê¸°ë¡œ ì§ˆë¬¸ì„ ë˜ì ¸ ì‘ë‹µì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    config = {"configurable": {"thread_id": f"eval_{user_id}"}}
    input_data = {"messages": [HumanMessage(content=question)]}
    
    final_state = await app_graph.ainvoke(input_data, config=config)
    ai_msg = next((m for m in reversed(final_state["messages"]) if isinstance(m, AIMessage)), None)
    
    if not ai_msg:
        return "ì‘ë‹µ ì—†ìŒ"
        
    if isinstance(ai_msg.content, str):
        return ai_msg.content
    elif isinstance(ai_msg.content, list):
        return "".join([part.get("text", "") for part in ai_msg.content if isinstance(part, dict)])
    return str(ai_msg.content)

async def measure_metric_with_retry(metric, test_case, retries=3):
    """Gemini API 429 ì—ëŸ¬(Rate Limit) ë°œìƒ ì‹œ ëŒ€ê¸° í›„ ì¬ì‹œë„í•˜ëŠ” ë˜í¼ í•¨ìˆ˜"""
    for attempt in range(retries):
        try:
            await metric.a_measure(test_case)
            return
        except Exception as e:
            error_str = str(e)
            if "429" in error_str or "RESOURCE_EXHAUSTED" in error_str:
                # ì½˜ì†”ì— ë®ì–´ì“°ê¸° í˜•íƒœë¡œ ì ì‹œ ëŒ€ê¸°ë¥¼ ì•Œë¦¼
                if attempt < retries - 1:
                    await asyncio.sleep(20) # ë¬´ë£Œ í‹°ì–´ API í• ë‹¹ëŸ‰ ë¦¬ì…‹ ëŒ€ê¸°
                else:
                    raise e
            else:
                raise e

async def main():
    console.print(Panel("[bold green]ğŸš€ ìš”ê¸ˆ ì•ˆë‚´ AI ëª¨ë¸ (LLM as a Judge) ìë™ ì±„ì  íŒŒì´í”„ë¼ì¸[/bold green]", expand=False))
    
    with console.status("[bold cyan]ğŸ“¥ Supabaseì—ì„œ í‰ê°€ ë°ì´í„°ì…‹ ì¡°íšŒ ì¤‘...[/bold cyan]"):
        dataset = fetch_eval_data_from_db()

    if not dataset:
        console.print("[bold red]âŒ í‰ê°€í•  ë°ì´í„°ê°€ DBì— ì—†ìŠµë‹ˆë‹¤.[/bold red]")
        return

    eval_subset = dataset[:5]
    console.print(f"âœ… í‰ê°€ ì§„í–‰ ë¬¸í•­ ìˆ˜: [bold yellow]{len(eval_subset)}[/bold yellow]ê±´\n")
    
    total_scaled_score = 0
    results_log = []
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        "[progress.percentage]{task.percentage:>3.0f}%",
        TimeElapsedColumn(),
        console=console,
        transient=False
    ) as progress:
        
        main_task = progress.add_task("[bold cyan]ì „ì²´ í‰ê°€ ì§„í–‰ë¥ ", total=len(eval_subset))
        
        for idx, data in enumerate(eval_subset):
            q_id = data.get("id", str(idx))
            question = data["question"]
            expected_ans = data["expected_answer"]
            context_refs = data.get("context_references", [])
            
            # API ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ê³ ì • ë”œë ˆì´ (ë¬´ë£Œ í‹°ì–´ 15 RPM ê³ ë ¤)
            if idx > 0:
                progress.update(main_task, description=f"[cyan]ë¬¸í•­ {idx+1}/{len(eval_subset)} ì¤€ë¹„ ì¤‘ (API ì¿¨ë‹¤ìš´ ëŒ€ê¸°)...")
                await asyncio.sleep(15)

            progress.update(main_task, description=f"[cyan]ë¬¸í•­ {idx+1}/{len(eval_subset)} í‰ê°€ ì¤‘: [white]{question[:15]}...")
            
            # ì‹œìŠ¤í…œì— ì§ˆë¬¸ ë˜ì§€ê¸° (ë¹„ë™ê¸°)
            actual_answer = await generate_agent_response(question, q_id)
            
            # DeepEvalìš© LLMTestCase ìƒì„±
            test_case = LLMTestCase(
                input=question,
                actual_output=actual_answer,
                expected_output=expected_ans,
                retrieval_context=[json.dumps(context_refs)] if context_refs else ["No explicit context provided"]
            )
            
            try:
                # ê° ë©”íŠ¸ë¦­ë§ˆë‹¤ Rate limit 429 íšŒí”¼ìš© ì¬ì‹œë„ í•¨ìˆ˜ ì‚¬ìš© ë° ì•½ê°„ì˜ ë”œë ˆì´
                await measure_metric_with_retry(groundedness_metric, test_case)
                await asyncio.sleep(3)
                
                await measure_metric_with_retry(evidenceability_metric, test_case)
                await asyncio.sleep(3)
                
                await measure_metric_with_retry(clarity_metric, test_case)
                await asyncio.sleep(3)
                
                await measure_metric_with_retry(atomicity_metric, test_case)
                await asyncio.sleep(3)
                
                await measure_metric_with_retry(robustness_metric, test_case)
                
                # ê°€ì¤‘ì¹˜ ê³„ì‚° ì²´ê³„
                g_score = groundedness_metric.score * 25
                e_score = evidenceability_metric.score * 15
                c_score = clarity_metric.score * 10
                a_score = atomicity_metric.score * 10
                r_score = robustness_metric.score * 5
                
                item_score = g_score + e_score + c_score + a_score + r_score
                scaled_100 = (item_score / 65) * 100
                total_scaled_score += scaled_100
                
                results_log.append({
                    "id": q_id,
                    "score": scaled_100,
                    "Grd": g_score, "Evi": e_score, "Cla": c_score, "Ato": a_score, "Rob": r_score
                })
                
            except Exception as e:
                console.print(f"[bold red]âŒ ì±„ì  ì˜¤ë¥˜ ë°œìƒ (ë¬¸í•­ {idx+1}): {e}[/bold red]")
                
            # í‰ê°€ í•œ ê±´ ì™„ë£Œë§ˆë‹¤ ê²Œì´ì§€ ë°” ì±„ìš°ê¸°
            progress.advance(main_task)

    # í‰ê°€ ê²°ê³¼ í…Œì´ë¸” UI êµ¬ì„±
    print("\n")
    table = Table(title="ğŸ“Š ë¬¸í•­ ë‹¨ìœ„ ê°œë³„ í‰ê°€ ê²°ê³¼ ìš”ì•½", show_header=True, header_style="bold magenta")
    table.add_column("ë¬¸í•­ ID", style="dim", width=10)
    table.add_column("ì´ì (100)", justify="right", style="bold green")
    table.add_column("ì •í•©(25)", justify="right")
    table.add_column("ì¦ê±°(15)", justify="right")
    table.add_column("ëª…í™•(10)", justify="right")
    table.add_column("ì›ì(10)", justify="right")
    table.add_column("ê°•ê±´(5)", justify="right")

    for log in results_log:
        table.add_row(
            str(log["id"])[:8],
            f"{log['score']:.1f}",
            f"{log['Grd']:.1f}", f"{log['Evi']:.1f}", f"{log['Cla']:.1f}", f"{log['Ato']:.1f}", f"{log['Rob']:.1f}"
        )

    console.print(table)
    
    if results_log:
        final_avg = total_scaled_score / len(results_log)
        console.print(Panel(f"[bold gold1]ğŸ† ë¯¸ë‹ˆ ë°°ì¹˜(5ê±´) ìµœì¢… í‰ê·  í‰ê°€ ì ìˆ˜: {final_avg:.1f} / 100ì [/bold gold1]", expand=False))

if __name__ == "__main__":
    asyncio.run(main())
